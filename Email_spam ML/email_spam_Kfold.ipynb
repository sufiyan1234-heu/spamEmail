{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv',encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df = df.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"] , axis=1)\n",
    "df = df.rename(columns={\"v1\":\"label\",\"v2\":\"text\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['text'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  length\n",
       "0      0  Go until jurong point, crazy.. Available only ...     111\n",
       "1      0                      Ok lar... Joking wif u oni...      29\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3      0  U dun say so early hor... U c already then say...      49\n",
       "4      0  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'label']=df.label.map({'ham':0,'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "text = count.fit_transform(df['text'])\n",
    "input = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "input_transformed = count.transform(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for MNB\n",
      "Best F1 Score: 0.9634146341463415\n",
      "Best Confusion Matrix:\n",
      "[[472   1]\n",
      " [  5  79]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       473\n",
      "           1       0.99      0.94      0.96        84\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       0.99      0.97      0.98       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.989247   0.969231  0.940299  0.954545\n",
      "1  0.978495   0.986301  0.867470  0.923077\n",
      "2  0.985637   0.984375  0.900000  0.940299\n",
      "3  0.985637   0.949367  0.949367  0.949367\n",
      "4  0.989228   0.948718  0.973684  0.961039\n",
      "5  0.983842   0.984375  0.887324  0.933333\n",
      "6  0.989228   1.000000  0.921053  0.958904\n",
      "7  0.989228   0.969697  0.941176  0.955224\n",
      "8  0.989228   0.987500  0.940476  0.963415\n",
      "9  0.980251   0.942857  0.904110  0.923077\n",
      "\n",
      "Mean Accuracy: 0.9860022007297156\n",
      "Mean Precision: 0.972242128897344\n",
      "Mean Recall: 0.9224958511461109\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = mnb.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for MNB\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = mnb.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(mnb,open('./kfold/mnb_kfold.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for SVC\n",
      "Best F1 Score: 0.48366013071895425\n",
      "Best Confusion Matrix:\n",
      "[[441  45]\n",
      " [ 34  37]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       486\n",
      "           1       0.45      0.52      0.48        71\n",
      "\n",
      "    accuracy                           0.86       557\n",
      "   macro avg       0.69      0.71      0.70       557\n",
      "weighted avg       0.87      0.86      0.86       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.853047   0.402597  0.462687  0.430556\n",
      "1  0.831541   0.400000  0.265060  0.318841\n",
      "2  0.829443   0.349398  0.414286  0.379085\n",
      "3  0.843806   0.447368  0.430380  0.438710\n",
      "4  0.831239   0.381579  0.381579  0.381579\n",
      "5  0.858169   0.451220  0.521127  0.483660\n",
      "6  0.818671   0.313433  0.276316  0.293706\n",
      "7  0.859964   0.419355  0.382353  0.400000\n",
      "8  0.834829   0.448718  0.416667  0.432099\n",
      "9  0.820467   0.324675  0.342466  0.333333\n",
      "\n",
      "Mean Accuracy: 0.8381176682560826\n",
      "Mean Precision: 0.3938342821498869\n",
      "Mean Recall: 0.3892919127922472\n",
      "Predictions: [0]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "    svc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = svc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for SVC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = svc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "pickle.dump(svc,open('./kfold/svc_kfold.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for KNC\n",
      "Best F1 Score: 0.6851851851851851\n",
      "Best Confusion Matrix:\n",
      "[[486   0]\n",
      " [ 34  37]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97       486\n",
      "           1       1.00      0.52      0.69        71\n",
      "\n",
      "    accuracy                           0.94       557\n",
      "   macro avg       0.97      0.76      0.83       557\n",
      "weighted avg       0.94      0.94      0.93       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.939068        1.0  0.492537  0.660000\n",
      "1  0.899642        1.0  0.325301  0.490909\n",
      "2  0.931777        1.0  0.457143  0.627451\n",
      "3  0.929982        1.0  0.506329  0.672269\n",
      "4  0.919210        1.0  0.407895  0.579439\n",
      "5  0.938959        1.0  0.521127  0.685185\n",
      "6  0.921005        1.0  0.421053  0.592593\n",
      "7  0.924596        1.0  0.382353  0.553191\n",
      "8  0.897666        1.0  0.321429  0.486486\n",
      "9  0.913824        1.0  0.342466  0.510204\n",
      "\n",
      "Mean Accuracy: 0.9215729426072856\n",
      "Mean Precision: 1.0\n",
      "Mean Recall: 0.4177631884333152\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    knc = KNeighborsClassifier()\n",
    "\n",
    "    knc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = knc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for KNC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = knc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(knc,open('./kfold/knc_kfold.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for DTC\n",
      "Best F1 Score: 0.8374999999999999\n",
      "Best Confusion Matrix:\n",
      "[[464  14]\n",
      " [ 12  67]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       478\n",
      "           1       0.83      0.85      0.84        79\n",
      "\n",
      "    accuracy                           0.95       557\n",
      "   macro avg       0.90      0.91      0.91       557\n",
      "weighted avg       0.95      0.95      0.95       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.962366   0.910714  0.761194  0.829268\n",
      "1  0.944444   0.919355  0.686747  0.786207\n",
      "2  0.940754   0.803279  0.700000  0.748092\n",
      "3  0.953321   0.827160  0.848101  0.837500\n",
      "4  0.942549   0.854839  0.697368  0.768116\n",
      "5  0.944345   0.822581  0.718310  0.766917\n",
      "6  0.942549   0.940000  0.618421  0.746032\n",
      "7  0.947935   0.842105  0.705882  0.768000\n",
      "8  0.942549   0.882353  0.714286  0.789474\n",
      "9  0.942549   0.859649  0.671233  0.753846\n",
      "\n",
      "Mean Accuracy: 0.9463362998140319\n",
      "Mean Precision: 0.8662034988755805\n",
      "Mean Recall: 0.7121542560403699\n",
      "Predictions: [0]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    dtc = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "    dtc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = dtc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for DTC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = dtc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(dtc,open('./kfold/dtc_kfold.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for LRC\n",
      "Best F1 Score: 0.9710144927536231\n",
      "Best Confusion Matrix:\n",
      "[[486   0]\n",
      " [  4  67]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       486\n",
      "           1       1.00      0.94      0.97        71\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       1.00      0.97      0.98       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.983871   1.000000  0.865672  0.928000\n",
      "1  0.967742   0.957746  0.819277  0.883117\n",
      "2  0.982047   0.983871  0.871429  0.924242\n",
      "3  0.983842   0.926829  0.962025  0.944099\n",
      "4  0.985637   0.972222  0.921053  0.945946\n",
      "5  0.992819   1.000000  0.943662  0.971014\n",
      "6  0.980251   0.971014  0.881579  0.924138\n",
      "7  0.982047   0.983333  0.867647  0.921875\n",
      "8  0.976661   0.938272  0.904762  0.921212\n",
      "9  0.973070   0.953125  0.835616  0.890511\n",
      "\n",
      "Mean Accuracy: 0.9807986332310186\n",
      "Mean Precision: 0.9686413368155309\n",
      "Mean Recall: 0.8872721590829\n",
      "Predictions: [0]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# input = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "# input_transformed = count_vectorizer.transform(input)\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "\n",
    "\n",
    "    lrc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = lrc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for LRC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "# lrc.predict(input_transformed)\n",
    "\n",
    "pickle.dump(lrc,open('lrc_KFold.pkl','wb'))\n",
    "\n",
    "input_text = [\"you win 5000$ in luck draw\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = lrc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "pickle.dump(count_vectorizer,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(lrc,open('./kfold/lrc_kfold.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for RFC\n",
      "Best F1 Score: 0.9395973154362416\n",
      "Best Confusion Matrix:\n",
      "[[478   0]\n",
      " [  9  70]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       478\n",
      "           1       1.00      0.89      0.94        79\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.99      0.94      0.97       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.982079        1.0  0.850746  0.919355\n",
      "1  0.967742        1.0  0.783133  0.878378\n",
      "2  0.980251        1.0  0.842857  0.914729\n",
      "3  0.983842        1.0  0.886076  0.939597\n",
      "4  0.978456        1.0  0.842105  0.914286\n",
      "5  0.978456        1.0  0.830986  0.907692\n",
      "6  0.974865        1.0  0.815789  0.898551\n",
      "7  0.978456        1.0  0.823529  0.903226\n",
      "8  0.978456        1.0  0.857143  0.923077\n",
      "9  0.976661        1.0  0.821918  0.902256\n",
      "\n",
      "Mean Accuracy: 0.9779264235568169\n",
      "Mean Precision: 1.0\n",
      "Mean Recall: 0.8354282620463234\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "\n",
    "\n",
    "    rfc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rfc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for RFC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = rfc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(rfc,open('./kfold/rfc_kfold.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for ABC\n",
      "Best F1 Score: 0.9565217391304348\n",
      "Best Confusion Matrix:\n",
      "[[485   1]\n",
      " [  5  66]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       486\n",
      "           1       0.99      0.93      0.96        71\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       0.99      0.96      0.98       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.974910   0.981818  0.805970  0.885246\n",
      "1  0.967742   0.933333  0.843373  0.886076\n",
      "2  0.964093   0.903226  0.800000  0.848485\n",
      "3  0.976661   0.934211  0.898734  0.916129\n",
      "4  0.978456   0.944444  0.894737  0.918919\n",
      "5  0.989228   0.985075  0.929577  0.956522\n",
      "6  0.969479   0.983607  0.789474  0.875912\n",
      "7  0.974865   0.921875  0.867647  0.893939\n",
      "8  0.969479   0.913580  0.880952  0.896970\n",
      "9  0.965889   0.950000  0.780822  0.857143\n",
      "\n",
      "Mean Accuracy: 0.9730803137648565\n",
      "Mean Precision: 0.9451168723519663\n",
      "Mean Recall: 0.8491287169133475\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "    abc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = abc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for ABC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = abc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "pickle.dump(abc,open('./kfold/abc_kfold.pkl','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for BC\n",
      "Best F1 Score: 0.9393939393939394\n",
      "Best Confusion Matrix:\n",
      "[[488   3]\n",
      " [  5  62]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       491\n",
      "           1       0.95      0.93      0.94        67\n",
      "\n",
      "    accuracy                           0.99       558\n",
      "   macro avg       0.97      0.96      0.97       558\n",
      "weighted avg       0.99      0.99      0.99       558\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.985663   0.953846  0.925373  0.939394\n",
      "1  0.962366   0.907895  0.831325  0.867925\n",
      "2  0.962298   0.901639  0.785714  0.839695\n",
      "3  0.980251   0.935897  0.924051  0.929936\n",
      "4  0.976661   0.909091  0.921053  0.915033\n",
      "5  0.982047   0.942029  0.915493  0.928571\n",
      "6  0.973070   0.969231  0.828947  0.893617\n",
      "7  0.973070   0.934426  0.838235  0.883721\n",
      "8  0.974865   0.926829  0.904762  0.915663\n",
      "9  0.973070   0.983333  0.808219  0.887218\n",
      "\n",
      "Mean Accuracy: 0.9743360810280368\n",
      "Mean Precision: 0.9364217165811126\n",
      "Mean Recall: 0.8683172688867078\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = bc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for BC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = bc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "pickle.dump(bc,open('./kfold/bc_kfold.pkl','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for ETC\n",
      "Best F1 Score: 0.9664429530201343\n",
      "Best Confusion Matrix:\n",
      "[[480   1]\n",
      " [  4  72]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       481\n",
      "           1       0.99      0.95      0.97        76\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       0.99      0.97      0.98       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.989247   1.000000  0.910448  0.953125\n",
      "1  0.971326   0.985507  0.819277  0.894737\n",
      "2  0.983842   1.000000  0.871429  0.931298\n",
      "3  0.983842   1.000000  0.886076  0.939597\n",
      "4  0.991023   0.986301  0.947368  0.966443\n",
      "5  0.983842   1.000000  0.873239  0.932331\n",
      "6  0.978456   1.000000  0.842105  0.914286\n",
      "7  0.987433   1.000000  0.897059  0.945736\n",
      "8  0.976661   1.000000  0.845238  0.916129\n",
      "9  0.973070   1.000000  0.794521  0.885496\n",
      "\n",
      "Mean Accuracy: 0.9818742237923335\n",
      "Mean Precision: 0.9971808616239825\n",
      "Mean Recall: 0.8686759977966382\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    etc.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = etc.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for ETC\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = etc.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(etc,open('./kfold/etc_kfold.pkl','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for GBDT\n",
      "Best F1 Score: 0.8750000000000001\n",
      "Best Confusion Matrix:\n",
      "[[476   2]\n",
      " [ 16  63]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       478\n",
      "           1       0.97      0.80      0.88        79\n",
      "\n",
      "    accuracy                           0.97       557\n",
      "   macro avg       0.97      0.90      0.93       557\n",
      "weighted avg       0.97      0.97      0.97       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.967742   0.980392  0.746269  0.847458\n",
      "1  0.949821   0.966102  0.686747  0.802817\n",
      "2  0.958707   0.912281  0.742857  0.818898\n",
      "3  0.967684   0.969231  0.797468  0.875000\n",
      "4  0.965889   0.901408  0.842105  0.870748\n",
      "5  0.965889   0.919355  0.802817  0.857143\n",
      "6  0.960503   0.965517  0.736842  0.835821\n",
      "7  0.969479   0.981132  0.764706  0.859504\n",
      "8  0.965889   1.000000  0.773810  0.872483\n",
      "9  0.953321   0.943396  0.684932  0.793651\n",
      "\n",
      "Mean Accuracy: 0.9624923585773761\n",
      "Mean Precision: 0.953881415544316\n",
      "Mean Recall: 0.7578552324797031\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Transform text data into numerical features\n",
    "    X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "    gbdt.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = gbdt.predict(X_test_transformed)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for GBDT\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = gbdt.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(gbdt,open('./kfold/gbdt_kfold.pkl','wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold for MNB\n",
      "Best F1 Score: 0.9605263157894737\n",
      "Best Confusion Matrix:\n",
      "[[478   3]\n",
      " [  3  73]]\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       481\n",
      "           1       0.96      0.96      0.96        76\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       0.98      0.98      0.98       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Results Table:\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.980287   0.878378  0.970149  0.921986\n",
      "1  0.978495   0.961039  0.891566  0.925000\n",
      "2  0.978456   0.902778  0.928571  0.915493\n",
      "3  0.985637   0.938272  0.962025  0.950000\n",
      "4  0.987433   0.936709  0.973684  0.954839\n",
      "5  0.983842   0.930556  0.943662  0.937063\n",
      "6  0.989228   0.960526  0.960526  0.960526\n",
      "7  0.983842   0.904110  0.970588  0.936170\n",
      "8  0.982047   0.940476  0.940476  0.940476\n",
      "9  0.962298   0.817073  0.917808  0.864516\n",
      "\n",
      "Mean Accuracy: 0.9811564126818659\n",
      "Mean Precision: 0.9169916404486903\n",
      "Mean Recall: 0.9459057406913874\n",
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform text data into numerical features\n",
    "X_transformed = count_vectorizer.fit_transform(X)\n",
    "\n",
    "# Initialize variables for best F1 score and confusion matrix\n",
    "best_f1_score = 0\n",
    "best_confusion_matrix = None\n",
    "best_classification_report = None\n",
    "\n",
    "# Initialize lists for storing metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X_transformed):\n",
    "    X_train, X_test = X_transformed[train_index], X_transformed[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the classifier\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = mnb.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Check if current fold has a better F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_confusion_matrix = cm\n",
    "        best_classification_report = report\n",
    "\n",
    "print(\"KFold for MNB\")\n",
    "\n",
    "# Print the best F1 score, confusion matrix, and classification report\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_confusion_matrix)\n",
    "print(\"Best Classification Report:\")\n",
    "print(best_classification_report)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Create a table for the results\n",
    "results_table = pd.DataFrame({\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print mean metrics\n",
    "print(\"\\nMean Accuracy:\", mean_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)\n",
    "\n",
    "# Make predictions on new input\n",
    "input_text = [\"Congratulations! You have won a cash prize of $1,000,000. To claim your prize, reply to this email with your full name, address, and bank account details. Act quickly to secure your winnings.\"]\n",
    "\n",
    "input_transformed = count_vectorizer.transform(input_text)\n",
    "predictions = mnb.predict(input_transformed)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "pickle.dump(mnb,open('./kfold/mnb_kfold.pkl','wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
